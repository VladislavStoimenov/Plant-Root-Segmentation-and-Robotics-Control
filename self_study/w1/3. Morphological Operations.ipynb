{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological operations are crucial for processing binary images as well as extract information from them.\n",
    "\n",
    "1. Dilation and Erosion\n",
    "2. Opening and Closing\n",
    "3. Skeletonization\n",
    "4. Connected Components\n",
    "5. Watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dilation and Erosion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilation in image processing is a morphological operation that expands the boundaries of objects in a binary image by adding pixels to the object's edges, making the objects thicker and larger. Erosion, on the other hand, shrinks the boundaries of objects by removing pixels from the object's edges, resulting in thinner and smaller objects in the binary image.\n",
    "\n",
    "In `OpenCV`, we have `cv2.dilate()` and `cv2.erode()` to perform these operations.\n",
    "\n",
    "Both methods use the same parameters: \n",
    "\n",
    "`cv2.erode(src, kernel, iterations)`\n",
    "\n",
    "`cv2.dilate(src, kernel, iterations)`\n",
    "\n",
    "- src: the image we would like to erode/dilate\n",
    "- kernel: is the kernel to be used for the operation\n",
    "- iterations: number of times the operation is applied\n",
    "\n",
    "check the documentation for other parameters.\n",
    "\n",
    "These methods return the eroded/dilated images. Let's create a circle and apply these operations to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a circle\n",
    "im_size = 100\n",
    "x = int(im_size/2)\n",
    "y = int(im_size/2)\n",
    "r = 40\n",
    "\n",
    "im_circle = np.zeros((im_size, im_size), dtype=\"uint8\")\n",
    "im_circle = cv2.circle(im_circle, (x, y), r, 255, -1)\n",
    "\n",
    "plt.imshow(im_circle, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erosion\n",
    "kernel = np.ones((5, 5), dtype=\"uint8\")\n",
    "im_erosion = cv2.erode(im_circle, kernel, iterations=1)\n",
    "plt.imshow(im_erosion, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dilation\n",
    "kernel = np.ones((5, 5), dtype=\"uint8\")\n",
    "im_dilation = cv2.dilate(im_circle, kernel, iterations=1)\n",
    "plt.imshow(im_dilation, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that indeed erosion made the circle smaller and dilation made the circle larger. Let's stack all three to see the effect more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.stack((im_dilation, im_erosion, im_circle), axis=2)\n",
    "plt.imshow(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The border between the red and magenta regions is the perimeter of the original circle. Red region represents the area added to the circle after the dilation operation. Magenta region represents the area eroded area after the erosion. The difference between dilation and erosion is called the morphological gradient (magenta + red). So far, so good.\n",
    "\n",
    "Now let's see what happens if we change the kernel size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller kernel\n",
    "kernel = np.ones((3, 3), dtype=\"uint8\") # (3, 3) instead of (5, 5)\n",
    "im_erosion = cv2.erode(im_circle, kernel, iterations=1)\n",
    "im_dilation = cv2.dilate(im_circle, kernel, iterations=1)\n",
    "stacked = np.stack((im_dilation, im_erosion, im_circle), axis=2)\n",
    "plt.imshow(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger kernel\n",
    "kernel = np.ones((7, 7), dtype=\"uint8\") # (7, 7) instead of (5, 5)\n",
    "im_erosion = cv2.erode(im_circle, kernel, iterations=1)\n",
    "im_dilation = cv2.dilate(im_circle, kernel, iterations=1)\n",
    "stacked = np.stack((im_dilation, im_erosion, im_circle), axis=2)\n",
    "plt.imshow(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller kernel resulted in less dilation/erosion and larger kernel resulted in more dilation/erosion. What about the iterations? If we set the iterations to 2, this means we repeat the operation twice, which should result in more dilation and erosion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5, 5), dtype=\"uint8\")\n",
    "im_erosion = cv2.erode(im_circle, kernel, iterations=2) # iterations 2 instead of 1\n",
    "im_dilation = cv2.dilate(im_circle, kernel, iterations=2) # iterations 2 instead of 1\n",
    "stacked = np.stack((im_dilation, im_erosion, im_circle), axis=2)\n",
    "plt.imshow(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, more iterations more erosion/dilation. Now, I know what you are thinking \"What if we combine these operations?\", let me answer that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**\n",
    "\n",
    "Apply erosion and dilation to the blobs images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_blobs = skimage.data.binary_blobs()\n",
    "plt.imshow(im_blobs, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Opening and Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilation and erosion act in opposite directions. But they are not invertible, i.e. when you apply erosion followed by dilation to an image, there is no guarentee that you obtain the same image. Same for dilation followed by erosion. Let's see what happens when we combine these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dilation + Erosion\n",
    "kernel = np.ones((5, 5), dtype=\"uint8\")\n",
    "im_d = cv2.dilate(im_circle, kernel, iterations=1)\n",
    "im_de = cv2.erode(im_d, kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's compare the resulting image (`im_de`) and the original image (`im_circle`) pixel by pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(im_de == im_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the original image when we applied Dilation + Erosion. Let's check Erosion + Dilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erosion + Dilation\n",
    "kernel = np.ones((5, 5), dtype=\"uint8\")\n",
    "im_e = cv2.erode(im_circle, kernel, iterations=1)\n",
    "im_ed = cv2.dilate(im_e, kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(im_ed == im_circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {np.sum(im_ed != im_circle)} pixels that are different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we didn't get the same image. It is still close but not exactly the same. 4 pixels are different, let's visualize them, look at the blue pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.stack((im_ed, im_ed, im_circle), axis=2)\n",
    "plt.imshow(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the point of all this? Yes we can chain the operations, so what? These operations are very useful as we will demonstrate below. In fact, they have their own names:\n",
    "\n",
    "- Opening: Erosion + Dilation\n",
    "- Closing: Dilation + Erosion\n",
    "\n",
    "Opening removes small objects/noise, or protrusions on the objects. Closing can be used to remove small intrusions and holes on the object. It can also be used to connect objects that are nearly touching.\n",
    "\n",
    "In fact when we applied opening above we eliminated the protrusions (the blue pixels). Let's look at a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we have an image of a rod. We used a neural network to find the pixels that belong to the rod. Our NN is good enough but not perfect. We obtained the following segmentation mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 100\n",
    "im_rod = np.zeros((im_size, im_size), dtype=\"uint8\")\n",
    "im_rod[:, 45:56] = 255\n",
    "im_rod[10, 45:56] = 0\n",
    "im_rod[80, 45:56] = 0\n",
    "plt.imshow(im_rod, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that there is a single rod in the image but in the segmentation mask we see a discontinuity. Let's apply opening and closing to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opening_closing_plot(im_rod, kernel = np.ones((5, 5), dtype=\"uint8\")):\n",
    "    # Opening\n",
    "    im_rod_e = cv2.erode(im_rod, kernel, iterations=1)\n",
    "    im_rod_opening = cv2.dilate(im_rod_e, kernel, iterations=1)\n",
    "\n",
    "    # Closing\n",
    "    im_rod_d = cv2.dilate(im_rod, kernel, iterations=1)\n",
    "    im_rod_closing = cv2.erode(im_rod_d, kernel, iterations=1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4)) \n",
    "\n",
    "    axes[0].imshow(im_rod, cmap='gray')\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    axes[1].imshow(im_rod_opening, cmap='gray')\n",
    "    axes[1].set_title('Opening')\n",
    "\n",
    "    axes[2].imshow(im_rod_closing, cmap='gray')\n",
    "    axes[2].set_title('Closing')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_closing_plot(im_rod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed closing can connect objects that are nearly touching. Opening did nothing in this case. What about intrusions and holes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new image with a hole and a notch\n",
    "im_size = 100\n",
    "im_rod = np.zeros((im_size, im_size), dtype=\"uint8\")\n",
    "im_rod[:, 45:56] = 255\n",
    "im_rod[10:12, 49:51] = 0 # hole\n",
    "im_rod[80:83, 45:48] = 0 # notch\n",
    "\n",
    "opening_closing_plot(im_rod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing closes ðŸ˜¬ðŸ˜¬ What if we have a few pixels detected as the rod but actually not the rod + a protrusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new image with protrusion and noise\n",
    "im_size = 100\n",
    "im_rod = np.zeros((im_size, im_size), dtype=\"uint8\")\n",
    "im_rod[:, 45:56] = 255\n",
    "im_rod[10:12, 10:12] = 255 # noise\n",
    "im_rod[80:84, 80:84] = 255 # noise\n",
    "im_rod[30:32, 56:58] = 255 # protrusion\n",
    "\n",
    "opening_closing_plot(im_rod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this time closing didn't help but opening did. Now you know dilation, erosion and their combination. It is important to understand what they do so that you can use them effectively in different situations you will encounter. You can even combine opening and closing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2**\n",
    "\n",
    "Apply erosion, dilation, opening, closing to `im_blobs` in order to:\n",
    "\n",
    "- remove white pixels in the background (salt)\n",
    "- remove black pixels on the blobs (pepper)\n",
    "- connecting blobs\n",
    "- disconnecting blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_blobs = skimage.data.binary_blobs(volume_fraction=0.3) # creating binary blobs image\n",
    "im_blobs = skimage.util.random_noise(im_blobs, mode='s&p') # adding salt and pepper noise\n",
    "plt.imshow(im_blobs, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Skeletonization ðŸ’€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the skeleton of an image is called skeletonization. What is a skeleton? You might guess what it is by its name, it is best understood by looking at an example. We will use `scikit-image`. Let's use another sample image from `scikit-image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_horse = skimage.data.horse()\n",
    "im_horse = 1 - im_horse # invert\n",
    "plt.imshow(im_horse, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and apply skeletonization to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import skeletonize\n",
    "skeleton = skeletonize(im_horse)\n",
    "plt.imshow(skeleton, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image skeleton, also known as a medial axis or topological skeleton, is a representation of an object within an image that captures its essential shape and connectivity. It is a one-pixel-wide representation that highlights the central and critical features of the object while simplifying its complex boundary, making it useful for tasks like shape analysis, object recognition, and pattern matching in image processing. We will come back to skeletons in week 4 to learn how we can use it for object measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3**\n",
    "\n",
    "Apply skeletonization to `blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_blobs = skimage.data.binary_blobs(volume_fraction=0.3) # creating binary blobs image\n",
    "plt.imshow(im_blobs, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connected Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a binary image with two circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_circles = np.zeros((20, 20), dtype=\"uint8\")\n",
    "im_circles = cv2.circle(im_circles, (5, 5), 3, 255, -1)\n",
    "im_circles = cv2.circle(im_circles, (15, 15), 3, 255, -1)\n",
    "\n",
    "plt.imshow(im_circles, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes I know these are not circles but bear with me, seeing individual pixels is helpful for understanding what is going on.\n",
    "\n",
    "We know that pixels with a value of 0 belong to the background and pixels with a value of 255 belong to the circles. But which white pixels belong to which instance of circle? In a sense, we are looking for pixels that are neighbors to each other i.e. _connected_.\n",
    "\n",
    "We can use [connected components](https://en.wikipedia.org/wiki/Component_(graph_theory)) from graph theory to find which pixels are \"connected\" and from there infer which pixels belong to the same object. We can do this in one line with `OpenCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval, labels = cv2.connectedComponents(im_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retval` is the number of components detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 3 components but we have 2 objects, how come? Background is also a connected component.\n",
    "\n",
    "Then, what is `labels`? This is a NumPy array of the same shape as the input binary image. It assigns a unique integer label to each pixel in the input image, indicating the connected component to which the pixel belongs. Pixels with the same label belong to the same connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we see that different objects have different shades! Let's plot label values to make this even more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.imshow(labels, cmap='gray')\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    for j in range(labels.shape[1]):\n",
    "        ax.text(j, i, str(labels[i, j]), color='r', ha='center', va='center', size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The background is assigned the label `0` and other connected components or objects are labeled with subsequent integers. In our case `1` and `2`. This is a common labeling convention in image segmentation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization use any color map that works best for your case\n",
    "plt.imshow(labels, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.4**\n",
    "\n",
    "Find and visualize the connected components in `im_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_blobs = skimage.data.binary_blobs(volume_fraction=0.3)\n",
    "plt.imshow(im_blobs, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool so now you know connected components! But we skipped explaining a core concept. What is a neighbor? What does it mean to be connected?\n",
    "\n",
    "Consider the following simple 5x5 px image with a 3x3 px rectangle at the center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_image = np.zeros((5, 5), dtype='uint8')\n",
    "small_image[1:4, 1:4] = 255\n",
    "plt.imshow(small_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which pixels are the neighbors of the pixel at (2, 2)? Let's draw this better by showing each pixel using gridlines and draw connections between the pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_image = np.zeros((5, 5), dtype='uint8')\n",
    "small_image[1:4, 1:4] = 255\n",
    "plt.imshow(small_image, cmap='gray')\n",
    "plt.xticks(np.arange(0.5, 5, 1), labels=['0.5', '1.5', '2.5', '3.5', '4.5'])\n",
    "plt.yticks(np.arange(0.5, 5, 1), labels=['0.5', '1.5', '2.5', '3.5', '4.5'])\n",
    "plt.grid(True, color='red', linewidth=2)\n",
    "plt.plot([2.1, 3], [2, 2], color='green')\n",
    "plt.plot([1, 1.9], [2, 2], color='green')\n",
    "plt.plot([2, 2], [1, 1.9], color='green')\n",
    "plt.plot([2, 2], [2.1, 3], color='green')\n",
    "plt.plot([2.1, 3], [2.1, 3], color='blue')\n",
    "plt.plot([1, 1.9], [3, 2.1], color='blue')\n",
    "plt.plot([1, 1.9], [1, 1.9], color='blue')\n",
    "plt.plot([2.1, 3], [1.9, 1], color='blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Green lines connect the 4 pixels that share an edge with the center pixel, blue lines connect the 4 pixels that share a corner with the center pixel. If we consider only edge connectivity center pixel has 4 neighbors. If we consider edge and corner connectivity, the center pixel has 8 neighbors. What about a black and a white pixel with edge connectivity? Are they connected? Nope. Only neighbors of same intensity are connected.\n",
    "\n",
    "Which connectivity does `OpenCV` use, 4 or 8? By default it uses 8. But we can change it. Let's see it on another image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "# Create image\n",
    "small_image = np.zeros((6, 6), dtype='uint8')\n",
    "small_image[1:3, 1:3] = 255\n",
    "small_image[3:5, 3:5] = 255\n",
    "# Display image\n",
    "ax[0].imshow(small_image, cmap='gray')\n",
    "ax[0].set_title('Image')\n",
    "\n",
    "# Connectivity 8 (Default)\n",
    "retval, labels = cv2.connectedComponents(small_image, connectivity=8) # default\n",
    "\n",
    "ax[1].imshow(labels, cmap='gray')\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    for j in range(labels.shape[1]):\n",
    "        ax[1].text(j, i, str(labels[i, j]), color='r', ha='center', va='center', size=8)\n",
    "ax[1].set_title('connectivity=8')\n",
    "\n",
    "# Connectivity 4\n",
    "retval, labels = cv2.connectedComponents(small_image, connectivity=4)\n",
    "\n",
    "ax[2].imshow(labels, cmap='gray')\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    for j in range(labels.shape[1]):\n",
    "        ax[2].text(j, i, str(labels[i, j]), color='r', ha='center', va='center', size=8)\n",
    "ax[2].set_title('connectivity=4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `labels` to find object area (in terms of number of pixels), object center, and a bounding box for the object. Luckily, `OpenCV` does that for us:\n",
    "\n",
    "`retval, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image)`\n",
    "\n",
    "you know the first two, the next two gives us more information on the connected components:\n",
    "\n",
    "- retval: The total number of connected components found in the input binary image.\n",
    "\n",
    "- labels: This is a NumPy array of the same shape as the input binary image. It assigns a unique integer label to each pixel in the input image, indicating the connected component to which the pixel belongs. Pixels with the same label belong to the same connected component.\n",
    "\n",
    "- stats: This is a NumPy array of shape (N, 5), where N is the number of connected components found. Each row of the stats array corresponds to a connected component and contains the following statistics:\n",
    "    - stats[i, 0]: Leftmost (x) coordinate of the bounding box of the i-th connected component.\n",
    "    - stats[i, 1]: Topmost (y) coordinate of the bounding box of the i-th connected component.\n",
    "    - stats[i, 2]: Width of the bounding box of the i-th connected component.\n",
    "    - stats[i, 3]: Height of the bounding box of the i-th connected component.\n",
    "    - stats[i, 4]: Total number of pixels in the i-th connected component.\n",
    "    \n",
    "\n",
    "- centroids: This is a NumPy array of shape (N, 2), where N is the number of connected components found. Each row of the centroids array corresponds to a connected component and contains the (x, y) coordinates of the centroid of that component.\n",
    "\n",
    "Let's run `cv2.connectedComponentsWithStats()` on proper circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_circles = np.zeros((400, 400), dtype=\"uint8\")\n",
    "im_circles = cv2.circle(im_circles, (100, 120), 30, 255, -1)\n",
    "im_circles = cv2.circle(im_circles, (300, 200), 60, 255, -1)\n",
    "\n",
    "plt.imshow(im_circles, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval, labels, stats, centroids = cv2.connectedComponentsWithStats(im_circles)\n",
    "plt.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, w, h, area\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a connect component. Row index corresponds to the label in `labels`. We can use (x, y, w, h) to draw bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(stats)):\n",
    "    x, y, w, h, _ = stats[i]\n",
    "    im_circles = cv2.rectangle(im_circles, (x, y), (x+w, y+h), 150, 1)\n",
    "plt.imshow(im_circles, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "area column can be used to filter components by size. `centroids` contain the centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.5**\n",
    "\n",
    "Detect the blobs in the image below. You will see that sometimes two blobs are connected by a very thin connection. It is up to you to consider them as separate objects or not.\n",
    "\n",
    "- How many blobs are there?\n",
    "- What is the average blob size?\n",
    "- Draw bounding boxes around the blobs\n",
    "\n",
    "Note that blob generation is random, which means if you repeat blob image generation, you will get a new image. This is useful for testing your solution on different images. So test the robustness of your solution on different blobs images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = skimage.data.binary_blobs(volume_fraction=0.3) # creating binary blobs image\n",
    "blobs = skimage.util.random_noise(blobs, mode='s&p') # adding noise\n",
    "plt.imshow(blobs, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the two objects are overlapping? If we apply connected components, they will have the same label because they are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two overlapping circles\n",
    "im_circles = np.zeros((400, 400), dtype=\"uint8\")\n",
    "im_circles = cv2.circle(im_circles, (240, 135), 30, 255, -1)\n",
    "im_circles = cv2.circle(im_circles, (300, 200), 60, 255, -1)\n",
    "\n",
    "plt.imshow(im_circles, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watershed algorithm is widely used to separate overlapping objects. It is used to find a boundary between overlapping objects. The way it works require a sure background (sure_bg), sure foreground (sure_fg) and an unknown region in between the two.\n",
    "\n",
    "**Sure_bg**: If you apply the `dilate` method to `im_circles` you will have a foreground wider than it is supposed to be. This means remaining background is for sure the background.\n",
    "\n",
    "**Sure_fg**: If you apply the `erode` method to `im_circles` you will have a foreground smaller than it is supposed to be and without overlapping. So you are sure this is the foreground.\n",
    "\n",
    "**Unknown region**: And finally `sure_bg - sure_fg` will result in the unknown region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((7, 7), dtype=\"uint8\")\n",
    "sure_bg = cv2.dilate(im_circles, kernel, iterations=3)\n",
    "sure_fg = cv2.erode(im_circles, kernel, iterations=3) # make sure erosion separates the two\n",
    "unknown = sure_bg - sure_fg\n",
    "\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "ax[0, 0].imshow(im_circles, cmap='gray')\n",
    "ax[0, 0].set_title('Original')\n",
    "ax[0, 0].axis('off')\n",
    "ax[0, 1].imshow(sure_bg, cmap='gray')\n",
    "ax[0, 1].set_title('Sure Background')\n",
    "ax[0, 1].axis('off')\n",
    "ax[1, 0].imshow(sure_fg, cmap='gray')\n",
    "ax[1, 0].set_title('Sure Foreground')\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 1].imshow(unknown, cmap='gray')\n",
    "ax[1, 1].set_title('Unknown')\n",
    "ax[1, 1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we need to do is to create `labels` such that the unknown region has a label `0`, the background `1` and the sure foregrounds `2` and `3`. Here are the steps do achieve that:\n",
    "\n",
    "1. Apply connected components to `sure_fg`. We will have background `0` and objects `1` and `2`.\n",
    "2. Add `1` to all labels in order to set background to `1` (instead of `0`).\n",
    "3. Update `labels` such that the unknown region is labeled as `0`.\n",
    "4. Apply `cv2.watershed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "retval, labels, stats, centroids = cv2.connectedComponentsWithStats(sure_fg)\n",
    "plt.imshow(labels)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Check the colobar\n",
    "labels = labels + 1\n",
    "plt.imshow(labels)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "labels[unknown == 255] = 0\n",
    "plt.imshow(labels)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV wants a 3-channel image for watershed\n",
    "im_circles = np.stack((im_circles, im_circles, im_circles), axis=2)\n",
    "im_circles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "labels = cv2.watershed(im_circles, labels)\n",
    "plt.imshow(labels)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked ðŸ¥³ We have different labels for overlapping objects ðŸ¥³ ðŸ¥³  But what is `-1` in the colorbar? It is the border between the objects. You can zoom in to see it clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomed_in = labels[150:190, 230:270]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.imshow(zoomed_in)\n",
    "\n",
    "for i in range(zoomed_in.shape[0]):\n",
    "    for j in range(zoomed_in.shape[1]):\n",
    "        ax.text(j, i, str(zoomed_in[i, j]), color='r', ha='center', va='center', size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.6**\n",
    "\n",
    "Use watershed to detect the two rectangles below. Find the bounding boxes from the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rectangles = np.zeros((400, 400), dtype=\"uint8\")\n",
    "im_rectangles[100:150, 100:200] = 255\n",
    "im_rectangles[140:180, 200:220] = 255\n",
    "plt.imshow(im_rectangles, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.7**\n",
    "\n",
    "Read the following example to see how watershed works with scikit-image.\n",
    "\n",
    "https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
